{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from scipy import spatial  \n",
    "from sklearn.metrics import r2_score\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from itertools import tee, islice\n",
    "from sklearn.decomposition import PCA\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_users(tweets):\n",
    "\n",
    "    list_of_user = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        user_id = tweet.user.id\n",
    "        if user_id not in list_of_user:\n",
    "            list_of_user.append(user_id)\n",
    "\n",
    "    return list_of_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Tweet and Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you want to save the coverted files # TO BE REMOVED\n",
    "test_directory = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of all the \".p\" files containing tweets of friends\n",
    "directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/\" \n",
    "\n",
    "# Location of sentiment analysis file\n",
    "sentiment_directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/Sentiment/\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets output from the function, get_inital_tweets\n",
    "initial_tweet_file = directory + 'user_tweet_large.p'\n",
    "initial_tweets = pickle.load(open(initial_tweet_file, \"rb\"))\n",
    "trending_tweet_file = directory + 'trending_tweets.p'\n",
    "trending_tweets = pickle.load(open(trending_tweet_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n"
     ]
    }
   ],
   "source": [
    "print(len(trending_tweets))\n",
    "list_of_users = get_list_of_users(trending_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use this if using above\n",
    "already_analyzed = pickle.load(open(analyzed_users, \"rb\"))\n",
    "list_of_users = []\n",
    "\n",
    "for uid in already_analyzed:\n",
    "    list_of_users.append(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM - Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/Sentiment/\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\"\n",
    "\n",
    "\n",
    "data_point_n_4 = []\n",
    "data_point_n_3 = []\n",
    "data_point_n_2 = []\n",
    "data_point_n_1 = []\n",
    "data_point_n_0 = []\n",
    "data_point_p_1 = []\n",
    "data_point_p_2 = []\n",
    "data_point_p_3 = []\n",
    "data_point_p_4 = []\n",
    "\n",
    "already_analyzed = pickle.load(open(analyzed_users, \"rb\"))\n",
    "list_of_users = []\n",
    "\n",
    "for uid in already_analyzed:\n",
    "    list_of_users.append(uid)\n",
    "\n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users:\n",
    "    \n",
    "    file_name = sentiment_directory + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "\n",
    "            if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "                    friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                    \n",
    "                    if friend_sentiment > 0:\n",
    "                        positive += 1\n",
    "                    elif friend_sentiment < 0:\n",
    "                        negative += 1\n",
    "                    else:\n",
    "                        neutral += 1\n",
    "                        \n",
    "                    friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                    friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                    if friend_sentiment_pos > 1:\n",
    "                        contain_pos += 1\n",
    "                        \n",
    "                    if friend_sentiment_neg < -1:\n",
    "                        contain_neg += 1\n",
    "                            \n",
    "                total = positive + negative + neutral\n",
    "                \n",
    "                if total > 0:\n",
    "                    positive_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent] #[cont_pos_percent, cont_neg_percent, neutral_percent]#[positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    if tweet_sent == -4:\n",
    "                        data_point_n_4.append(temp)\n",
    "                    elif tweet_sent == -3:\n",
    "                        data_point_n_3.append(temp)\n",
    "                    elif tweet_sent == -2:\n",
    "                        data_point_n_2.append(temp)\n",
    "                    elif tweet_sent == -1:\n",
    "                        data_point_n_1.append(temp)\n",
    "                    elif tweet_sent == 0:\n",
    "                        data_point_n_0.append(temp)\n",
    "                    elif tweet_sent == 1:\n",
    "                        data_point_p_1.append(temp)\n",
    "                    elif tweet_sent == 2:\n",
    "                        data_point_p_2.append(temp)\n",
    "                    elif tweet_sent == 3:\n",
    "                        data_point_p_3.append(temp)\n",
    "                    elif tweet_sent == 4:\n",
    "                        data_point_p_4.append(temp)\n",
    "\n",
    "data_point_n_4 = np.asarray(data_point_n_4)\n",
    "data_point_n_3 = np.asarray(data_point_n_3)\n",
    "data_point_n_2 = np.asarray(data_point_n_2)\n",
    "data_point_n_1 = np.asarray(data_point_n_1)\n",
    "data_point_n_0 = np.asarray(data_point_n_0)\n",
    "data_point_p_1 = np.asarray(data_point_p_1)\n",
    "data_point_p_2 = np.asarray(data_point_p_2)\n",
    "data_point_p_3 = np.asarray(data_point_p_3)\n",
    "data_point_p_4 = np.asarray(data_point_p_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.1468254  0.51984127 0.         1.        ]\n",
      " [0.41666667 0.         0.58333333 0.         1.        ]\n",
      " [0.66666667 0.         0.33333333 0.         1.        ]\n",
      " ...\n",
      " [0.         0.         1.         1.         0.        ]\n",
      " [0.33333333 0.         0.66666667 1.         0.        ]\n",
      " [0.24847251 0.13849287 0.61303462 1.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma='auto_deprecated',\n",
       "    kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import random\n",
    "\n",
    "resample_index = 0\n",
    "\n",
    "data_point_n_4 = data_point_n_4.tolist()\n",
    "data_point_n_3 = data_point_n_3.tolist()\n",
    "data_point_n_2 = data_point_n_2.tolist()\n",
    "data_point_n_1 = data_point_n_1.tolist()\n",
    "data_point_n_0 = data_point_n_0.tolist()\n",
    "data_point_p_1 = data_point_p_1.tolist()\n",
    "data_point_p_2 = data_point_p_2.tolist()\n",
    "data_point_p_3 = data_point_p_3.tolist()\n",
    "data_point_p_4 = data_point_p_4.tolist()\n",
    "\n",
    "n_4_label = np.zeros(len(data_point_n_4))\n",
    "n_3_label = np.zeros(len(data_point_n_3))\n",
    "n_2_label = np.zeros(len(data_point_n_2))\n",
    "n_1_label = np.zeros(len(data_point_n_1))\n",
    "n_0_label = np.zeros(len(data_point_n_0))\n",
    "p_1_label = np.zeros(len(data_point_p_1))\n",
    "p_2_label = np.zeros(len(data_point_p_2))\n",
    "p_3_label = np.zeros(len(data_point_p_3))\n",
    "p_4_label = np.zeros(len(data_point_p_4))\n",
    "\n",
    "max_length = max(len(n_4_label), len(n_3_label), len(n_2_label), len(n_1_label), len(n_0_label), len(p_1_label), len(p_2_label), len(p_3_label), len(p_4_label))\n",
    "\n",
    "for i in range(max_length):\n",
    "    try:\n",
    "        n_4_label[i] = -4\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        n_3_label[i] = -3\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        n_2_label[i] = -2\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        n_1_label[i] = -1\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        p_1_label[i] = 1\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        p_2_label[i] = 2\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        p_3_label[i] = 3\n",
    "    except:\n",
    "        a = 1\n",
    "    try:\n",
    "        p_4_label[i] = 4\n",
    "    except:\n",
    "        a = 1\n",
    "        \n",
    "n_4_label = n_4_label.tolist()\n",
    "n_3_label = n_3_label.tolist()\n",
    "n_2_label = n_2_label.tolist()\n",
    "n_1_label = n_1_label.tolist()\n",
    "n_0_label = n_0_label.tolist()\n",
    "p_1_label = p_1_label.tolist()\n",
    "p_2_label = p_2_label.tolist()\n",
    "p_3_label = p_3_label.tolist()\n",
    "p_4_label = p_4_label.tolist()\n",
    "\n",
    "''' \n",
    "data_point_n_4 = data_point_n_4.tolist()\n",
    "data_point_n_3 = data_point_n_3.tolist()\n",
    "data_point_n_2 = data_point_n_2.tolist()\n",
    "data_point_n_1 = data_point_n_1.tolist()\n",
    "data_point_n_0 = data_point_n_0.tolist()\n",
    "data_point_p_1 = data_point_p_1.tolist()\n",
    "data_point_p_2 = data_point_p_2.tolist()\n",
    "data_point_p_3 = data_point_p_3.tolist()\n",
    "data_point_p_4 = data_point_p_4.tolist()\n",
    "'''\n",
    "\n",
    "\n",
    "all_labels = n_4_label + n_3_label + n_2_label + n_1_label + n_0_label + p_1_label + p_2_label + p_3_label + p_4_label\n",
    "all_labels = np.asarray(all_labels)\n",
    "\n",
    "all_points = data_point_n_4 + data_point_n_3 + data_point_n_2 + data_point_n_1 + data_point_n_0 + data_point_p_1 + data_point_p_2 + data_point_p_3 + data_point_p_4\n",
    "all_points = np.asarray(all_points)\n",
    "print(all_points)\n",
    "\n",
    "clf = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo') #svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo')  #svm.SVC(kernel='poly', degree=4, C=1, decision_function_shape='ovo') #svm.SVC(kernel='linear', C=1, decision_function_shape='ovo') #svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo') #svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo') #svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo') #svm.NuSVC(gamma='auto') #kernel='poly', probability=True\n",
    "clf.fit(all_points, all_labels)\n",
    "#clf.fit(np.asarray(data_point_p_4), np.asarray(p_4_label), sample_weight=20)\n",
    "#tech = clf.predict([[0.2, 0.3, 0.5, 0.5, 0.5]])\n",
    "#print(int(tech[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[-4. -4. -4. ...  4.  4.  4.]\n"
     ]
    }
   ],
   "source": [
    "tech = clf.predict([[0.24847251, 0.13849287, 0.61303462, 1,0]])\n",
    "print(int(tech[0]))\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Prep for Testing Model\n",
    "result = {}\n",
    "\n",
    "NZ_tweet_dir = predictive_model = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Partner Code/\"\n",
    "predictive_model = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Partner Code/Sentiment/\"\n",
    "\n",
    "#tweet_file_name = predictive_model  + 'user_tweet_large_NZ_1.p'\n",
    "#predictive_user_tweets = pickle.load(open(tweet_file_name, \"rb\"))\n",
    "\n",
    "trending_tweet_file_NZ = NZ_tweet_dir + 'trending_tweets_NZ.p'\n",
    "trending_tweets_NZ = pickle.load(open(trending_tweet_file_NZ, \"rb\"))\n",
    "\n",
    "list_of_users_NZ = get_list_of_users(trending_tweets_NZ)\n",
    "\n",
    "analyzed_users_NZ = predictive_model  + 'data_analyzed_1.p'\n",
    "already_analyzed_NZ = pickle.load(open(analyzed_users_NZ, \"rb\"))\n",
    "\n",
    "for uid in already_analyzed_NZ:\n",
    "    if uid not in list_of_users_NZ:\n",
    "        list_of_users_NZ.append(uid)\n",
    "        \n",
    "print(len(list_of_users_NZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "total_count = 0\n",
    "accurate = 0\n",
    "\n",
    "n_4_total = 0\n",
    "n_4_accurate = 0\n",
    "n_4_miss = 0\n",
    "n_4_miss_list = []\n",
    "\n",
    "n_3_total = 0\n",
    "n_3_accurate = 0\n",
    "n_3_miss = 0\n",
    "n_3_miss_list = []\n",
    "\n",
    "n_2_total = 0\n",
    "n_2_accurate = 0\n",
    "n_2_miss = 0\n",
    "n_2_miss_list = []\n",
    "\n",
    "n_1_total = 0\n",
    "n_1_accurate = 0\n",
    "n_1_miss = 0\n",
    "n_1_miss_list = []\n",
    "\n",
    "n_0_total = 0\n",
    "n_0_accurate = 0\n",
    "n_0_miss = 0\n",
    "n_0_miss_list = []\n",
    "\n",
    "p_1_total = 0\n",
    "p_1_accurate = 0\n",
    "p_1_miss = 0\n",
    "p_1_miss_list = []\n",
    "\n",
    "p_2_total = 0\n",
    "p_2_accurate = 0\n",
    "p_2_miss = 0\n",
    "p_2_miss_list = []\n",
    "\n",
    "p_3_total = 0\n",
    "p_3_accurate = 0\n",
    "p_3_miss = 0\n",
    "p_3_miss_list = []\n",
    "\n",
    "p_4_total = 0\n",
    "p_4_accurate = 0\n",
    "p_4_miss = 0\n",
    "p_4_miss_list = []\n",
    "    \n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users_NZ:\n",
    "    \n",
    "    file_name = predictive_model + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "            \n",
    "            if tweet_sent != 100:\n",
    "\n",
    "                if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                    for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "\n",
    "                        friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                        if friend_sentiment > 0:\n",
    "                            positive += 1\n",
    "                        elif friend_sentiment < 0:\n",
    "                            negative += 1\n",
    "                        else:\n",
    "                            neutral += 1\n",
    "                            \n",
    "                        friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                        friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                        if friend_sentiment_pos > 1:\n",
    "                            contain_pos += 1\n",
    "                        \n",
    "                        if friend_sentiment_neg < -1:\n",
    "                            contain_neg += 1\n",
    "                            \n",
    "\n",
    "                total = positive + negative + neutral\n",
    "                \n",
    "                if total > 0:\n",
    "                    positve_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent] #[cont_pos_percent, cont_neg_percent, neutral_percent]#[positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    exemplar = int(clf.predict([temp])[0])\n",
    "                    total_count += 1\n",
    "                    \n",
    "                    if tweet_sent == exemplar:\n",
    "                        accurate += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate 4066\n",
      "Total Count 5164\n",
      "% Accurate 0.7873741285824942\n"
     ]
    }
   ],
   "source": [
    "print(\"Accurate \" + str(accurate))\n",
    "print(\"Total Count \" + str(total_count))\n",
    "print(\"% Accurate \" + str(accurate/total_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

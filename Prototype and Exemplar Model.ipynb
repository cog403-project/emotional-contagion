{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from scipy import spatial  \n",
    "from sklearn.metrics import r2_score\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from itertools import tee, islice\n",
    "from sklearn.decomposition import PCA\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_users(tweets):\n",
    "\n",
    "    list_of_user = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        user_id = tweet.user.id\n",
    "        if user_id not in list_of_user:\n",
    "            list_of_user.append(user_id)\n",
    "\n",
    "    return list_of_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Tweet and Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you want to save the coverted files # TO BE REMOVED\n",
    "test_directory = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of all the \".p\" files containing tweets of friends\n",
    "directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/\" \n",
    "\n",
    "# Location of sentiment analysis file\n",
    "sentiment_directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/Sentiment/\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets output from the function, get_inital_tweets\n",
    "initial_tweet_file = directory + 'user_tweet_large.p'\n",
    "initial_tweets = pickle.load(open(initial_tweet_file, \"rb\"))\n",
    "trending_tweet_file = directory + 'trending_tweets.p'\n",
    "trending_tweets = pickle.load(open(trending_tweet_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use this if using above\n",
    "already_analyzed = pickle.load(open(analyzed_users, \"rb\"))\n",
    "list_of_users = []\n",
    "\n",
    "for uid in already_analyzed:\n",
    "    list_of_users.append(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype and Exemplar Models - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code from COG260\n",
    "# Helper method for similarity function\n",
    "def similarity_function(feature_vector_0, feature_vector_1):\n",
    "    euclidean_distance = spatial.distance.euclidean(feature_vector_0, feature_vector_1)\n",
    "    temp_calc = euclidean_distance**2\n",
    "    similarity = np.exp(0 - temp_calc)\n",
    "    return similarity\n",
    "    \n",
    "    \n",
    "# Helper method for examplar model\n",
    "def categorize_with_exemplar(test_data_point, category_0, category_1, category_2):\n",
    "    \n",
    "    similarity_sum_cateogry_0 = []\n",
    "    similarity_sum_cateogry_1 = []\n",
    "    similarity_sum_cateogry_2 = []\n",
    "    \n",
    "    for object_0 in category_0:\n",
    "        similarity_sum_cateogry_0.append(similarity_function(object_0, test_data_point))\n",
    "        \n",
    "    for object_1 in category_1:\n",
    "        similarity_sum_cateogry_1.append(similarity_function(object_1, test_data_point))\n",
    "        \n",
    "    for object_2 in category_2:\n",
    "        similarity_sum_cateogry_2.append(similarity_function(object_2, test_data_point))\n",
    "        \n",
    "    similarity_sum_cateogry_0 = np.sum(similarity_sum_cateogry_0)\n",
    "    similarity_sum_cateogry_1 = np.sum(similarity_sum_cateogry_1)\n",
    "    similarity_sum_cateogry_2 = np.sum(similarity_sum_cateogry_2)\n",
    "    \n",
    "    similarity_0 = similarity_sum_cateogry_0 / len(category_0)\n",
    "    similarity_1 = similarity_sum_cateogry_1 / len(category_1)\n",
    "    similarity_2 = similarity_sum_cateogry_2 / len(category_2)\n",
    "    \n",
    "    combined = [similarity_0, similarity_1, similarity_2]\n",
    "    return_value = combined.index(max(combined))\n",
    "    #print(combined)\n",
    "    #print(return_value)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def categorize_with_2_exemplar(test_data_point, category_0, category_1):\n",
    "    \n",
    "    similarity_sum_cateogry_0 = []\n",
    "    similarity_sum_cateogry_1 = []\n",
    "    \n",
    "    for object_0 in category_0:\n",
    "        similarity_sum_cateogry_0.append(similarity_function(object_0, test_data_point))\n",
    "        \n",
    "    for object_1 in category_1:\n",
    "        similarity_sum_cateogry_1.append(similarity_function(object_1, test_data_point))\n",
    "        \n",
    "    similarity_sum_cateogry_0 = np.sum(similarity_sum_cateogry_0)\n",
    "    similarity_sum_cateogry_1 = np.sum(similarity_sum_cateogry_1)\n",
    "    \n",
    "    similarity_0 = similarity_sum_cateogry_0 / len(category_0)\n",
    "    similarity_1 = similarity_sum_cateogry_1 / len(category_1)\n",
    "\n",
    "    combined = [similarity_0, similarity_1]\n",
    "    return_value = combined.index(max(combined))\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to get the prototype\n",
    "def get_prototype(data_point):\n",
    "        \n",
    "    prototype = []\n",
    "        \n",
    "    for i in range(len(data_point[0, :])):\n",
    "        prototype.append(np.mean(data_point[:,i]))\n",
    "            \n",
    "    return prototype\n",
    "\n",
    "def categorize_with_prototype(test_data_point, category_0, category_1, category_2):\n",
    "    \n",
    "    cat_0_distance = spatial.distance.euclidean(test_data_point, category_0)\n",
    "    cat_1_distance = spatial.distance.euclidean(test_data_point, category_1)\n",
    "    cat_2_distance = spatial.distance.euclidean(test_data_point, category_2)\n",
    "    \n",
    "    combined = [cat_0_distance, cat_1_distance, cat_2_distance]\n",
    "    return_value = combined.index(min(combined))\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def categorize_with_2_prototype(test_data_point, category_0, category_1):\n",
    "    \n",
    "    cat_0_distance = spatial.distance.euclidean(test_data_point, category_0)\n",
    "    cat_1_distance = spatial.distance.euclidean(test_data_point, category_1)\n",
    "    \n",
    "    combined = [cat_0_distance, cat_1_distance]\n",
    "    return_value = combined.index(min(combined))\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype and Exemplar Model - Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/Sentiment/\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\"\n",
    "\n",
    "positive_x = []\n",
    "positive_y = []\n",
    "positive_z = []\n",
    "\n",
    "negative_x = []\n",
    "negative_y = []\n",
    "negative_z = []\n",
    "\n",
    "neutral_x = []\n",
    "neutral_y = []\n",
    "neutral_z = []\n",
    "\n",
    "positive_data_point = []\n",
    "negative_data_point = []\n",
    "neutral_data_point = []\n",
    "\n",
    "already_analyzed = pickle.load(open(analyzed_users, \"rb\"))\n",
    "list_of_users = []\n",
    "\n",
    "for uid in already_analyzed:\n",
    "    list_of_users.append(uid)\n",
    "\n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users:\n",
    "    \n",
    "    file_name = sentiment_directory + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "\n",
    "            if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "                    friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                    \n",
    "                    if friend_sentiment > 0:\n",
    "                        positive += 1\n",
    "                    elif friend_sentiment < 0:\n",
    "                        negative += 1\n",
    "                    else:\n",
    "                        neutral += 1\n",
    "                        \n",
    "                    friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                    friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                    if friend_sentiment_pos > 2:\n",
    "                        contain_pos += 1\n",
    "                        \n",
    "                    if friend_sentiment_neg < -1:\n",
    "                        contain_pos += 1\n",
    "                            \n",
    "                total = positive + negative + neutral\n",
    "                \n",
    "                if total > 0:\n",
    "                    positive_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    if tweet_sent > 0:\n",
    "                        positive_x.append(positive_percent)\n",
    "                        positive_y.append(negative_percent)\n",
    "                        positive_z.append(neutral_percent)\n",
    "\n",
    "                        positive_data_point.append(temp)\n",
    "                    elif tweet_sent < 0:\n",
    "                        negative_x.append(positive_percent)\n",
    "                        negative_y.append(negative_percent)\n",
    "                        negative_z.append(neutral_percent) \n",
    "\n",
    "                        negative_data_point.append(temp)\n",
    "                    else:\n",
    "                        neutral_x.append(positive_percent)\n",
    "                        neutral_y.append(negative_percent)\n",
    "                        neutral_z.append(neutral_percent)  \n",
    "\n",
    "                        neutral_data_point.append(temp)\n",
    "\n",
    "positive_data_point = np.asarray(positive_data_point)\n",
    "negative_data_point = np.asarray(negative_data_point)\n",
    "neutral_data_point = np.asarray(neutral_data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prototype\n",
    "\n",
    "positive_prototype = get_prototype(positive_data_point)\n",
    "negative_prototype = get_prototype(negative_data_point)\n",
    "neutral_prototype = get_prototype(neutral_data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Prep for Testing Model\n",
    "result = {}\n",
    "\n",
    "NZ_tweet_dir = predictive_model = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Partner Code/\"\n",
    "predictive_model = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Partner Code/Sentiment/\"\n",
    "\n",
    "#tweet_file_name = predictive_model  + 'user_tweet_large_NZ_1.p'\n",
    "#predictive_user_tweets = pickle.load(open(tweet_file_name, \"rb\"))\n",
    "\n",
    "trending_tweet_file_NZ = NZ_tweet_dir + 'trending_tweets_NZ.p'\n",
    "trending_tweets_NZ = pickle.load(open(trending_tweet_file_NZ, \"rb\"))\n",
    "\n",
    "list_of_users_NZ = get_list_of_users(trending_tweets_NZ)\n",
    "\n",
    "analyzed_users_NZ = predictive_model  + 'data_analyzed_1.p'\n",
    "already_analyzed_NZ = pickle.load(open(analyzed_users_NZ, \"rb\"))\n",
    "\n",
    "for uid in already_analyzed_NZ:\n",
    "    if uid not in list_of_users_NZ:\n",
    "        list_of_users_NZ.append(uid)\n",
    "        \n",
    "print(len(list_of_users_NZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype - only positive and negative\n",
    "\n",
    "total_count = 0\n",
    "accurate = 0\n",
    "\n",
    "p_total = 0\n",
    "p_acc = 0\n",
    "\n",
    "n_total = 0\n",
    "n_acc = 0\n",
    "\n",
    "ne_total = 0\n",
    "ne_acc = 0\n",
    "\n",
    "pos_total = 0\n",
    "neg_total = 0\n",
    "\n",
    "p_miss = 0\n",
    "n_miss = 0\n",
    "\n",
    "positive_total = 0\n",
    "negative_total = 0\n",
    "    \n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users_NZ:\n",
    "    \n",
    "    file_name = predictive_model + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "            \n",
    "            if tweet_sent != 0:\n",
    "\n",
    "                if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                    for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "\n",
    "                        friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                        if friend_sentiment > 0:\n",
    "                            positive += 1\n",
    "                        elif friend_sentiment < 0:\n",
    "                            negative += 1\n",
    "                        else:\n",
    "                            neutral += 1\n",
    "                            \n",
    "                        friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                        friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                        if friend_sentiment_pos > 2:\n",
    "                            contain_pos += 1\n",
    "                        \n",
    "                        if friend_sentiment_neg < -1:\n",
    "                            contain_pos += 1\n",
    "                            \n",
    "\n",
    "                total = positive + negative + neutral\n",
    "                \n",
    "                if total > 0:\n",
    "                    positve_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    exemplar = categorize_with_2_prototype(temp, positive_prototype, negative_prototype) \n",
    "                    total_count += 1\n",
    "                    \n",
    "                    if exemplar == 0:\n",
    "                        pos_total += 1\n",
    "                        \n",
    "                    if exemplar == 1:\n",
    "                        neg_total += 1\n",
    "\n",
    "                    if tweet_sent > 0:\n",
    "                        p_total += 1\n",
    "                        if exemplar == 0:\n",
    "                            accurate += 1\n",
    "                            p_acc += 1\n",
    "                        else:\n",
    "                            p_miss += 1\n",
    "                    elif tweet_sent < 0:\n",
    "                        n_total += 1\n",
    "                        if exemplar == 1:\n",
    "                            accurate += 1\n",
    "                            n_acc += 1\n",
    "                        else:\n",
    "                            n_miss += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accurate 2017\n",
      "Total Count 2581\n",
      "Positive Percent Predicted 0.6361290322580645\n",
      "Negative Percevent Predicted 1.0\n",
      "True Positive: 1.0\n",
      "False Positive: 0.0\n",
      "Missed Positive: 564\n",
      "True Negative: 0.6463949843260188\n",
      "False Negative: 0.35360501567398117\n",
      "Missed Negative: 0\n",
      "Positive Post 1550\n",
      "Negative Post 1031\n",
      "Positive Accurate Predict 986\n",
      "Negative Accurate Predict 1031\n",
      "# of positive prediction 986\n",
      "# of negative prediction 1595\n"
     ]
    }
   ],
   "source": [
    "# Result for Prototype Model\n",
    "print(\"Total Accurate \" + str(accurate))\n",
    "print(\"Total Count \" + str(total_count))\n",
    "print(\"Positive Percent Predicted \" + str(p_acc/p_total))\n",
    "print(\"Negative Percevent Predicted \" + str(n_acc/n_total))\n",
    "print(\"True Positive: \" + str(p_acc/pos_total))\n",
    "print(\"False Positive: \" + str((pos_total - p_acc)/pos_total))\n",
    "print(\"Missed Positive: \" + str(p_miss))\n",
    "print(\"True Negative: \" + str(n_acc/neg_total))\n",
    "print(\"False Negative: \" + str((neg_total - n_acc)/neg_total))\n",
    "print(\"Missed Negative: \" + str(n_miss))\n",
    "print(\"Positive Post \" + str(p_total))\n",
    "print(\"Negative Post \" + str(n_total))\n",
    "print(\"Positive Accurate Predict \"+ str(p_acc))\n",
    "print(\"Negative Accurate Predict \"+ str(n_acc))\n",
    "print(\"# of positive prediction \" + str(pos_total))\n",
    "print(\"# of negative prediction \" + str(neg_total))\n",
    "#print(ne_acc/ne_total)\n",
    "#print(accurate/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype\n",
    "\n",
    "total_count = 0\n",
    "accurate = 0\n",
    "\n",
    "p_total = 0\n",
    "p_acc = 0\n",
    "\n",
    "n_total = 0\n",
    "n_acc = 0\n",
    "\n",
    "ne_total = 0\n",
    "ne_acc = 0\n",
    "\n",
    "pos_total = 0\n",
    "neg_total = 0\n",
    "neu_total = 0\n",
    "\n",
    "p_miss = 0\n",
    "n_miss = 0\n",
    "ne_miss = 0\n",
    "    \n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users_NZ:\n",
    "    \n",
    "    file_name = predictive_model + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "            \n",
    "\n",
    "            if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "\n",
    "                    friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                    if friend_sentiment > 0:\n",
    "                        positive += 1\n",
    "                    elif friend_sentiment < 0:\n",
    "                        negative += 1\n",
    "                    else:\n",
    "                        neutral += 1\n",
    "                            \n",
    "                    friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                    friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                    if friend_sentiment_pos > 2:\n",
    "                        contain_pos += 1\n",
    "                        \n",
    "                    if friend_sentiment_neg < -1:\n",
    "                        contain_pos += 1\n",
    "                            \n",
    "\n",
    "            total = positive + negative + neutral\n",
    "            if total > 0:\n",
    "                positve_percent = positive/total\n",
    "                negative_percent = negative/total\n",
    "                neutral_percent = neutral/total\n",
    "                    \n",
    "                cont_pos_percent = contain_pos/total\n",
    "                cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                temp = np.asarray(temp)\n",
    "\n",
    "                exemplar = categorize_with_prototype(temp, positive_prototype, negative_prototype, neutral_prototype) \n",
    "                total_count += 1\n",
    "                    \n",
    "                if exemplar == 0:\n",
    "                    pos_total += 1\n",
    "                        \n",
    "                if exemplar == 1:\n",
    "                    neg_total += 1\n",
    "                    \n",
    "                if exemplar == 2:\n",
    "                    neu_total += 1\n",
    "\n",
    "                    if tweet_sent > 0:\n",
    "                        p_total += 1\n",
    "                        if exemplar == 0:\n",
    "                            accurate += 1\n",
    "                            p_acc += 1\n",
    "                        else:\n",
    "                            p_miss += 1\n",
    "                    elif tweet_sent < 0:\n",
    "                        n_total += 1\n",
    "                        if exemplar == 1:\n",
    "                            accurate += 1\n",
    "                            n_acc += 1\n",
    "                        else:\n",
    "                            n_miss += 1\n",
    "                    else:\n",
    "                        ne_total += 1\n",
    "                        if exemplar == 2:\n",
    "                            accurate += 1\n",
    "                            ne_acc += 1\n",
    "                        else:\n",
    "                            ne_miss += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accurate 2283\n",
      "Total Count 5164\n",
      "Neutral Percevent Predicted 1.0\n",
      "Missed Positive: 986\n",
      "Missed Negative: 0\n",
      "Missed Neutral: 0\n",
      "Positive Post 986\n",
      "Negative Post 0\n",
      "Neutral Post 2283\n",
      "Positive Accurate Predict 0\n",
      "Negative Accurate Predict 0\n",
      "Neutral Accurate Predict 2283\n",
      "# of positive prediction 0\n",
      "# of negative prediction 1895\n",
      "# of neutral prediction 3269\n"
     ]
    }
   ],
   "source": [
    "# Result for Prototype Model\n",
    "print(\"Total Accurate \" + str(accurate))\n",
    "print(\"Total Count \" + str(total_count))\n",
    "#print(\"Positive Percent Predicted \" + str(p_acc/p_total))\n",
    "#print(\"Negative Percevent Predicted \" + str(n_acc/n_total))\n",
    "print(\"Neutral Percevent Predicted \" + str(ne_acc/ne_total))\n",
    "#print(\"True Positive: \" + str(p_acc/pos_total))\n",
    "#print(\"False Positive: \" + str((pos_total - p_acc)/pos_total))\n",
    "print(\"Missed Positive: \" + str(p_miss))\n",
    "#print(\"True Negative: \" + str(n_acc/neg_total))\n",
    "#print(\"False Negative: \" + str((neg_total - n_acc)/neg_total))\n",
    "print(\"Missed Negative: \" + str(n_miss))\n",
    "#print(\"True Neutral: \" + str(ne_acc/neu_total))\n",
    "#print(\"False Neutral: \" + str((neu_total - ne_acc)/neu_total))\n",
    "print(\"Missed Neutral: \" + str(ne_miss))\n",
    "print(\"Positive Post \" + str(p_total))\n",
    "print(\"Negative Post \" + str(n_total))\n",
    "print(\"Neutral Post \" + str(ne_total))\n",
    "print(\"Positive Accurate Predict \"+ str(p_acc))\n",
    "print(\"Negative Accurate Predict \"+ str(n_acc))\n",
    "print(\"Neutral Accurate Predict \"+ str(ne_acc))\n",
    "print(\"# of positive prediction \" + str(pos_total))\n",
    "print(\"# of negative prediction \" + str(neg_total))\n",
    "print(\"# of neutral prediction \" + str(neu_total))\n",
    "#print(ne_acc/ne_total)\n",
    "#print(accurate/total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplar Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "accurate = 0\n",
    "\n",
    "p_total = 0\n",
    "p_acc = 0\n",
    "\n",
    "n_total = 0\n",
    "n_acc = 0\n",
    "\n",
    "ne_total = 0\n",
    "ne_acc = 0\n",
    "\n",
    "pos_total = 0\n",
    "neg_total = 0\n",
    "    \n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users_NZ:\n",
    "    \n",
    "    file_name = predictive_model + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "            \n",
    "            if tweet_sent != 0:\n",
    "\n",
    "                if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                    for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "\n",
    "                        friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                        if friend_sentiment > 0:\n",
    "                            positive += 1\n",
    "                        elif friend_sentiment < 0:\n",
    "                            negative += 1\n",
    "                        else:\n",
    "                            neutral += 1\n",
    "                            \n",
    "                        friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                        friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                        if friend_sentiment_pos > 2:\n",
    "                            contain_pos += 1\n",
    "                        \n",
    "                        if friend_sentiment_neg < -1:\n",
    "                            contain_pos += 1\n",
    "                            \n",
    "\n",
    "                total = positive + negative + neutral\n",
    "                if total > 0:\n",
    "                    positve_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    exemplar = categorize_with_2_exemplar(temp, positive_data_point, negative_data_point) \n",
    "                    total_count += 1\n",
    "                    \n",
    "                    if exemplar == 0:\n",
    "                        pos_total += 1\n",
    "                        \n",
    "                    if exemplar == 1:\n",
    "                        neg_total += 1\n",
    "\n",
    "                    if tweet_sent > 0:\n",
    "                        p_total += 1\n",
    "                        if exemplar == 0:\n",
    "                            accurate += 1\n",
    "                            p_acc += 1\n",
    "                    elif tweet_sent < 0:\n",
    "                        n_total += 1\n",
    "                        if exemplar == 1:\n",
    "                            accurate += 1\n",
    "                            n_acc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accurate 2017\n",
      "Total Count 2581\n",
      "Positive Percent Predicted 0.6361290322580645\n",
      "Negative Percevent Predicted 1.0\n",
      "True Positive: 1.0\n",
      "False Positive: 0.0\n",
      "Missed Positive: 564\n",
      "True Negative: 0.6463949843260188\n",
      "False Negative: 0.35360501567398117\n",
      "Missed Negative: 0\n",
      "Positive Post 1550\n",
      "Negative Post 1031\n",
      "Positive Accurate Predict 986\n",
      "Negative Accurate Predict 1031\n",
      "# of positive prediction 986\n",
      "# of negative prediction 1595\n"
     ]
    }
   ],
   "source": [
    "# Result for Examplar Model\n",
    "print(\"Total Accurate \" + str(accurate))\n",
    "print(\"Total Count \" + str(total_count))\n",
    "print(\"Positive Percent Predicted \" + str(p_acc/p_total))\n",
    "print(\"Negative Percevent Predicted \" + str(n_acc/n_total))\n",
    "print(\"True Positive: \" + str(p_acc/pos_total))\n",
    "print(\"False Positive: \" + str((pos_total - p_acc)/pos_total))\n",
    "print(\"Missed Positive: \" + str(p_miss))\n",
    "print(\"True Negative: \" + str(n_acc/neg_total))\n",
    "print(\"False Negative: \" + str((neg_total - n_acc)/neg_total))\n",
    "print(\"Missed Negative: \" + str(n_miss))\n",
    "print(\"Positive Post \" + str(p_total))\n",
    "print(\"Negative Post \" + str(n_total))\n",
    "print(\"Positive Accurate Predict \"+ str(p_acc))\n",
    "print(\"Negative Accurate Predict \"+ str(n_acc))\n",
    "print(\"# of positive prediction \" + str(pos_total))\n",
    "print(\"# of negative prediction \" + str(neg_total))\n",
    "#print(ne_acc/ne_total)\n",
    "#print(accurate/total_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from scipy import spatial  \n",
    "from sklearn.metrics import r2_score\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from itertools import tee, islice\n",
    "from sklearn.decomposition import PCA\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_users(tweets):\n",
    "\n",
    "    list_of_user = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        user_id = tweet.user.id\n",
    "        if user_id not in list_of_user:\n",
    "            list_of_user.append(user_id)\n",
    "\n",
    "    return list_of_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Tweet and Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you want to save the coverted files # TO BE REMOVED\n",
    "test_directory = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of all the \".p\" files containing tweets of friends\n",
    "directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/\" \n",
    "\n",
    "# Location of sentiment analysis file\n",
    "sentiment_directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/Sentiment/\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets output from the function, get_inital_tweets\n",
    "initial_tweet_file = directory + 'user_tweet_large.p'\n",
    "initial_tweets = pickle.load(open(initial_tweet_file, \"rb\"))\n",
    "trending_tweet_file = directory + 'trending_tweets.p'\n",
    "trending_tweets = pickle.load(open(trending_tweet_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use this if using above\n",
    "already_analyzed = pickle.load(open(analyzed_users, \"rb\"))\n",
    "list_of_users = []\n",
    "\n",
    "for uid in already_analyzed:\n",
    "    list_of_users.append(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype and Exemplar Models - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code from COG260\n",
    "# Helper method for similarity function\n",
    "def similarity_function(feature_vector_0, feature_vector_1):\n",
    "    euclidean_distance = spatial.distance.euclidean(feature_vector_0, feature_vector_1)\n",
    "    temp_calc = euclidean_distance**2\n",
    "    similarity = np.exp(0 - temp_calc)\n",
    "    return similarity\n",
    "    \n",
    "    \n",
    "# Helper method for examplar model\n",
    "def categorize_with_exemplar(test_data_point, category_0, category_1, category_2):\n",
    "    \n",
    "    similarity_sum_cateogry_0 = []\n",
    "    similarity_sum_cateogry_1 = []\n",
    "    similarity_sum_cateogry_2 = []\n",
    "    \n",
    "    for object_0 in category_0:\n",
    "        similarity_sum_cateogry_0.append(similarity_function(object_0, test_data_point))\n",
    "        \n",
    "    for object_1 in category_1:\n",
    "        similarity_sum_cateogry_1.append(similarity_function(object_1, test_data_point))\n",
    "        \n",
    "    for object_2 in category_2:\n",
    "        similarity_sum_cateogry_2.append(similarity_function(object_2, test_data_point))\n",
    "        \n",
    "    similarity_sum_cateogry_0 = np.sum(similarity_sum_cateogry_0)\n",
    "    similarity_sum_cateogry_1 = np.sum(similarity_sum_cateogry_1)\n",
    "    similarity_sum_cateogry_2 = np.sum(similarity_sum_cateogry_2)\n",
    "    \n",
    "    similarity_0 = similarity_sum_cateogry_0 / len(category_0)\n",
    "    similarity_1 = similarity_sum_cateogry_1 / len(category_1)\n",
    "    similarity_2 = similarity_sum_cateogry_2 / len(category_2)\n",
    "    \n",
    "    combined = [similarity_0, similarity_1, similarity_2]\n",
    "    return_value = combined.index(max(combined))\n",
    "    #print(combined)\n",
    "    #print(return_value)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def categorize_with_2_exemplar(test_data_point, category_0, category_1):\n",
    "    \n",
    "    similarity_sum_cateogry_0 = []\n",
    "    similarity_sum_cateogry_1 = []\n",
    "    \n",
    "    for object_0 in category_0:\n",
    "        similarity_sum_cateogry_0.append(similarity_function(object_0, test_data_point))\n",
    "        \n",
    "    for object_1 in category_1:\n",
    "        similarity_sum_cateogry_1.append(similarity_function(object_1, test_data_point))\n",
    "        \n",
    "    similarity_sum_cateogry_0 = np.sum(similarity_sum_cateogry_0)\n",
    "    similarity_sum_cateogry_1 = np.sum(similarity_sum_cateogry_1)\n",
    "    \n",
    "    similarity_0 = similarity_sum_cateogry_0 / len(category_0)\n",
    "    similarity_1 = similarity_sum_cateogry_1 / len(category_1)\n",
    "\n",
    "    combined = [similarity_0, similarity_1]\n",
    "    return_value = combined.index(max(combined))\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to get the prototype\n",
    "def get_prototype(data_point):\n",
    "        \n",
    "    prototype = []\n",
    "        \n",
    "    for i in range(len(data_point[0, :])):\n",
    "        prototype.append(np.mean(data_point[:,i]))\n",
    "            \n",
    "    return prototype\n",
    "\n",
    "def categorize_with_prototype(test_data_point, category_0, category_1, category_2):\n",
    "    \n",
    "    cat_0_distance = spatial.distance.euclidean(test_data_point, category_0)\n",
    "    cat_1_distance = spatial.distance.euclidean(test_data_point, category_1)\n",
    "    cat_2_distance = spatial.distance.euclidean(test_data_point, category_2)\n",
    "    \n",
    "    combined = [cat_0_distance, cat_1_distance, cat_2_distance]\n",
    "    return_value = combined.index(min(combined))\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def categorize_with_2_prototype(test_data_point, category_0, category_1):\n",
    "    \n",
    "    cat_0_distance = spatial.distance.euclidean(test_data_point, category_0)\n",
    "    cat_1_distance = spatial.distance.euclidean(test_data_point, category_1)\n",
    "    \n",
    "    combined = [cat_0_distance, cat_1_distance]\n",
    "    return_value = combined.index(min(combined))\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_with_score_prototype(test_data_point, category_0, category_1, category_2, category_3, category_4, category_5, category_6, category_7, category_8):\n",
    "    \n",
    "    cat_0_distance = spatial.distance.euclidean(test_data_point, category_0)\n",
    "    cat_1_distance = spatial.distance.euclidean(test_data_point, category_1)\n",
    "    cat_2_distance = spatial.distance.euclidean(test_data_point, category_2)\n",
    "    cat_3_distance = spatial.distance.euclidean(test_data_point, category_3)\n",
    "    cat_4_distance = spatial.distance.euclidean(test_data_point, category_4)\n",
    "    cat_5_distance = spatial.distance.euclidean(test_data_point, category_5)\n",
    "    cat_6_distance = spatial.distance.euclidean(test_data_point, category_6)\n",
    "    cat_7_distance = spatial.distance.euclidean(test_data_point, category_7)\n",
    "    cat_8_distance = spatial.distance.euclidean(test_data_point, category_8)\n",
    "    \n",
    "    combined = [cat_0_distance, cat_1_distance, cat_2_distance, cat_3_distance, cat_4_distance, cat_5_distance, cat_6_distance, cat_7_distance, cat_8_distance]\n",
    "    return_value = combined.index(min(combined))\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype and Exemplar Model - Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_directory = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/Sentiment/\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\"\n",
    "analyzed_users = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Python Tweets/data_analyzed_1.p\"\n",
    "\n",
    "\n",
    "data_point_n_4 = []\n",
    "data_point_n_3 = []\n",
    "data_point_n_2 = []\n",
    "data_point_n_1 = []\n",
    "data_point_n_0 = []\n",
    "data_point_p_1 = []\n",
    "data_point_p_2 = []\n",
    "data_point_p_3 = []\n",
    "data_point_p_4 = []\n",
    "\n",
    "already_analyzed = pickle.load(open(analyzed_users, \"rb\"))\n",
    "list_of_users = []\n",
    "\n",
    "for uid in already_analyzed:\n",
    "    list_of_users.append(uid)\n",
    "\n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users:\n",
    "    \n",
    "    file_name = sentiment_directory + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "\n",
    "            if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "                    friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                    \n",
    "                    if friend_sentiment > 0:\n",
    "                        positive += 1\n",
    "                    elif friend_sentiment < 0:\n",
    "                        negative += 1\n",
    "                    else:\n",
    "                        neutral += 1\n",
    "                        \n",
    "                    friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                    friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                    if friend_sentiment_pos > 1:\n",
    "                        contain_pos += 1\n",
    "                        \n",
    "                    if friend_sentiment_neg < -1:\n",
    "                        contain_neg += 1\n",
    "                            \n",
    "                total = positive + negative + neutral\n",
    "                \n",
    "                if total > 0:\n",
    "                    positive_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    if tweet_sent == -4:\n",
    "                        data_point_n_4.append(temp)\n",
    "                    elif tweet_sent == -3:\n",
    "                        data_point_n_3.append(temp)\n",
    "                    elif tweet_sent == -2:\n",
    "                        data_point_n_2.append(temp)\n",
    "                    elif tweet_sent == -1:\n",
    "                        data_point_n_1.append(temp)\n",
    "                    elif tweet_sent == 0:\n",
    "                        data_point_n_0.append(temp)\n",
    "                    elif tweet_sent == 1:\n",
    "                        data_point_p_1.append(temp)\n",
    "                    elif tweet_sent == 2:\n",
    "                        data_point_p_2.append(temp)\n",
    "                    elif tweet_sent == 3:\n",
    "                        data_point_p_3.append(temp)\n",
    "                    elif tweet_sent == 4:\n",
    "                        data_point_p_4.append(temp)\n",
    "\n",
    "data_point_n_4 = np.asarray(data_point_n_4)\n",
    "data_point_n_3 = np.asarray(data_point_n_3)\n",
    "data_point_n_2 = np.asarray(data_point_n_2)\n",
    "data_point_n_1 = np.asarray(data_point_n_1)\n",
    "data_point_n_0 = np.asarray(data_point_n_0)\n",
    "data_point_p_1 = np.asarray(data_point_p_1)\n",
    "data_point_p_2 = np.asarray(data_point_p_2)\n",
    "data_point_p_3 = np.asarray(data_point_p_3)\n",
    "data_point_p_4 = np.asarray(data_point_p_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prototype\n",
    "# Prototype chosen due to various studies that suggest proportion of post and whether \n",
    "# you have seen negativity \n",
    "#(i.e. not the overall sentiment, but whether there is any hint of negativity)\n",
    "\n",
    "prototype_n_1 = get_prototype(data_point_n_1)\n",
    "prototype_n_2 = get_prototype(data_point_n_2)\n",
    "prototype_n_3 = get_prototype(data_point_n_3)\n",
    "prototype_n_4 = get_prototype(data_point_n_4)\n",
    "prototype_n_0 = get_prototype(data_point_n_0)\n",
    "prototype_p_1 = get_prototype(data_point_p_1)\n",
    "prototype_p_2 = get_prototype(data_point_p_2)\n",
    "prototype_p_3 = get_prototype(data_point_p_3)\n",
    "prototype_p_4 = get_prototype(data_point_p_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5126910284070478, 0.12110319172616905, 0.3662057798667831, 0.0, 1.0]\n",
      "[0.3452308270471499, 0.14878076821321132, 0.5059884047396388, 0.022900763358778626, 1.0]\n",
      "[0.3378288067171415, 0.15970149014067128, 0.5024697031421872, 0.22821576763485477, 1.0]\n",
      "[0.3323557406348522, 0.15657513454269747, 0.5110691248224503, 0.1813953488372093, 1.0]\n",
      "[0.33347087874083997, 0.1481407023076638, 0.5183884189514961, 0.07017543859649122, 0.07017543859649122]\n",
      "[0.3445961375149226, 0.15148264489754074, 0.5039212175875366, 1.0, 0.06029285099052541]\n",
      "[0.3438294305887958, 0.1627417230819233, 0.4934288463292809, 1.0, 0.03579952267303103]\n",
      "[0.35276381460033973, 0.16165904346989957, 0.4855771419297607, 1.0, 0.0]\n",
      "[0.26724633140111753, 0.060264243563632566, 0.6724894250352498, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(prototype_n_4)\n",
    "print(prototype_n_3)\n",
    "print(prototype_n_2)\n",
    "print(prototype_n_1)\n",
    "print(prototype_n_0)\n",
    "print(prototype_p_1)\n",
    "print(prototype_p_2)\n",
    "print(prototype_p_3)\n",
    "print(prototype_p_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Prep for Testing Model\n",
    "result = {}\n",
    "\n",
    "NZ_tweet_dir = predictive_model = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Partner Code/\"\n",
    "predictive_model = \"L:/Users/Daniel/Documents/2020-2021/2021 Winter Courses/COG403/Assignments/Project/Partner Code/Sentiment/\"\n",
    "\n",
    "#tweet_file_name = predictive_model  + 'user_tweet_large_NZ_1.p'\n",
    "#predictive_user_tweets = pickle.load(open(tweet_file_name, \"rb\"))\n",
    "\n",
    "trending_tweet_file_NZ = NZ_tweet_dir + 'trending_tweets_NZ.p'\n",
    "trending_tweets_NZ = pickle.load(open(trending_tweet_file_NZ, \"rb\"))\n",
    "\n",
    "list_of_users_NZ = get_list_of_users(trending_tweets_NZ)\n",
    "\n",
    "analyzed_users_NZ = predictive_model  + 'data_analyzed_1.p'\n",
    "already_analyzed_NZ = pickle.load(open(analyzed_users_NZ, \"rb\"))\n",
    "\n",
    "for uid in already_analyzed_NZ:\n",
    "    if uid not in list_of_users_NZ:\n",
    "        list_of_users_NZ.append(uid)\n",
    "        \n",
    "print(len(list_of_users_NZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype - only positive and negative\n",
    "\n",
    "total_count = 0\n",
    "accurate = 0\n",
    "\n",
    "p_total = 0\n",
    "p_acc = 0\n",
    "\n",
    "n_total = 0\n",
    "n_acc = 0\n",
    "\n",
    "ne_total = 0\n",
    "ne_acc = 0\n",
    "\n",
    "pos_total = 0\n",
    "neg_total = 0\n",
    "\n",
    "p_miss = 0\n",
    "n_miss = 0\n",
    "\n",
    "positive_total = 0\n",
    "negative_total = 0\n",
    "\n",
    "score_index = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
    "    \n",
    "# Analyze all the  users in initial_tweets\n",
    "for user in list_of_users_NZ:\n",
    "    \n",
    "    file_name = predictive_model + str(user) + '_sent.p'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        \n",
    "        tweet_sentiment = pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "        for tweet_id in tweet_sentiment:\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            neutral = 0\n",
    "            contain_pos = 0\n",
    "            contain_neg = 0\n",
    "            \n",
    "            tweet_sent = tweet_sentiment[tweet_id]['combined']\n",
    "            \n",
    "            if tweet_sent != 0:\n",
    "\n",
    "                if 'friend' in tweet_sentiment[tweet_id]:\n",
    "                    for tweet in tweet_sentiment[tweet_id]['friend']:\n",
    "\n",
    "                        friend_sentiment = tweet_sentiment[tweet_id]['friend'][tweet]['combined']\n",
    "                        if friend_sentiment > 0:\n",
    "                            positive += 1\n",
    "                        elif friend_sentiment < 0:\n",
    "                            negative += 1\n",
    "                        else:\n",
    "                            neutral += 1\n",
    "                            \n",
    "                        friend_sentiment_pos = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][0]\n",
    "                        friend_sentiment_neg = tweet_sentiment[tweet_id]['friend'][tweet]['sentiment'][1]\n",
    "                    \n",
    "                        if friend_sentiment_pos > 1:\n",
    "                            contain_pos += 1\n",
    "                        \n",
    "                        if friend_sentiment_neg < -1:\n",
    "                            contain_neg += 1\n",
    "                            \n",
    "\n",
    "                total = positive + negative + neutral\n",
    "                \n",
    "                if total > 0:\n",
    "                    positve_percent = positive/total\n",
    "                    negative_percent = negative/total\n",
    "                    neutral_percent = neutral/total\n",
    "                    \n",
    "                    cont_pos_percent = contain_pos/total\n",
    "                    cont_neg_percent = contain_neg/total\n",
    "                    \n",
    "                    temp = [positive_percent, negative_percent, neutral_percent, cont_pos_percent, cont_neg_percent]\n",
    "                    temp = np.asarray(temp)\n",
    "\n",
    "                    exemplar = categorize_with_score_prototype(temp, prototype_n_4, prototype_n_3, prototype_n_2, prototype_n_1, prototype_n_0, prototype_p_1, prototype_p_2, prototype_p_3, prototype_p_4) \n",
    "                    exemplar = score_index[exemplar]\n",
    "                    total_count += 1\n",
    "                    \n",
    "                    \n",
    "                    #print(exemplar, tweet_sent)\n",
    "                    if exemplar == tweet_sent:\n",
    "                        accurate += 1\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "2581\n"
     ]
    }
   ],
   "source": [
    "print(accurate)\n",
    "print(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
